{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b45c99-60d4-4a9a-8803-e3ffbcb9d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82a17a-b04f-4e9c-8aeb-90afc71164b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/negar/Documents/Datasets/ChicagoWild++/mediapipe_res_chicago/\"\n",
    "hand_detected_label = \"/home/negar/Desktop/Pooya/TF-DeepHand/Transformer/sign_hand_detection_wild++_first.csv\"\n",
    "labels_csv = \"/home/negar/Desktop/Pooya/TF-DeepHand/Transformer/final.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593074ab-d78c-4604-a8da-5ff0d51f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 10\n",
    "char_counts = 32\n",
    "learning_rate = 0.0001\n",
    "optim_step_size = 10\n",
    "optim_gamma = 0.1\n",
    "num_epochs = 120\n",
    "SOS_token = 32\n",
    "EOS_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859aca15-8c83-4516-abf2-237b9bab537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_type = \"beam\"\n",
    "decode_type = \"trans\"\n",
    "\n",
    "beam_size  = 5\n",
    "lm_beta = 0.4\n",
    "ins_gamma = 1.2\n",
    "chars = \"$' &.@acbedgfihkjmlonqpsrutwvyxz\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b195d170-f52a-42b1-a9d6-c27ba9ea6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def beam_search(\n",
    "    model, \n",
    "    X,\n",
    "    poses,\n",
    "    predictions = 20,\n",
    "    beam_width = 5,\n",
    "    batch_size = 128, \n",
    "    progress_bar = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements Beam Search to extend the sequences given in X. The method can compute \n",
    "    several outputs in parallel with the first dimension of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------    \n",
    "    X: LongTensor of shape (examples, length)\n",
    "        The sequences to start the decoding process.\n",
    "\n",
    "    predictions: int\n",
    "        The number of tokens to append to X.\n",
    "\n",
    "    beam_width: int\n",
    "        The number of candidates to keep in the search.\n",
    "\n",
    "    batch_size: int\n",
    "        The batch size of the inner loop of the method, which relies on the beam width. \n",
    "\n",
    "    progress_bar: bool\n",
    "        Shows a tqdm progress bar, useful for tracking progress with large tensors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: LongTensor of shape (examples, length + predictions)\n",
    "        The sequences extended with the decoding process.\n",
    "\n",
    "    probabilities: FloatTensor of length examples\n",
    "        The estimated log-probabilities for the output sequences. They are computed by iteratively adding the \n",
    "        probability of the next token at every step.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # The next command can be a memory bottleneck, but can be controlled with the batch \n",
    "        # size of the predict method.\n",
    "\n",
    "        next_probabilities = model.forward(poses,X)[0][:,-1, :]\n",
    "        vocabulary_size = next_probabilities.shape[-1]\n",
    "        probabilities, idx = next_probabilities.squeeze().log_softmax(-1)\\\n",
    "            .topk(k = beam_width, axis = -1)\n",
    "        X = X.repeat((beam_width, 1, 1)).transpose(0, 1)\\\n",
    "            .flatten(end_dim = -2)\n",
    "        next_chars = idx.reshape(-1, 1)\n",
    "        X = torch.cat((X, next_chars), axis = -1)\n",
    "        \n",
    "        poses = torch.cat((poses,poses, poses, poses, poses), axis = 0)\n",
    "        # This has to be minus one because we already produced a round\n",
    "        # of predictions before the for loop.\n",
    "        predictions_iterator = range(predictions - 1)\n",
    "        if progress_bar > 0:\n",
    "            predictions_iterator = tqdm(predictions_iterator)\n",
    "        for i in predictions_iterator:\n",
    "            dataset = tud.TensorDataset(X)\n",
    "            loader = tud.DataLoader(dataset, batch_size = batch_size)\n",
    "            next_probabilities = []\n",
    "            iterator = iter(loader)\n",
    "            if progress_bar > 1:\n",
    "                iterator = tqdm(iterator)\n",
    "            for (x,) in iterator:\n",
    "                next_probabilities.append(\n",
    "                    model.forward(poses, x)[0][:,-1, :].log_softmax(-1)\n",
    "                )\n",
    "            next_probabilities = torch.cat(next_probabilities, axis = 0)\n",
    "            next_probabilities = next_probabilities.reshape(\n",
    "                (-1, beam_width, next_probabilities.shape[-1])\n",
    "            )\n",
    "            probabilities = probabilities.unsqueeze(-1) + next_probabilities\n",
    "            probabilities = probabilities.flatten(start_dim = 1)\n",
    "            probabilities, idx = probabilities.topk(\n",
    "                k = beam_width, \n",
    "                axis = -1\n",
    "            )\n",
    "            next_chars = torch.remainder(idx, vocabulary_size).flatten()\\\n",
    "                .unsqueeze(-1)\n",
    "            best_candidates = (idx / vocabulary_size).long()\n",
    "            best_candidates += torch.arange(\n",
    "                X.shape[0] // beam_width, \n",
    "                device = X.device\n",
    "            ).unsqueeze(-1) * beam_width\n",
    "            X = X[best_candidates].flatten(end_dim = -2)\n",
    "            X = torch.cat((X, next_chars), axis = 1)\n",
    "        return X.reshape(-1, beam_width, X.shape[-1]), probabilities\n",
    "\n",
    "class Decoder(object):\n",
    "    def __init__(self, labels, blank_index=0):\n",
    "        self.labels = labels\n",
    "        self.int_to_char = dict([(i, c) for (i, c) in enumerate(labels)])\n",
    "        self.char_to_int = dict([(c, i) for (i, c) in enumerate(labels)])\n",
    "\n",
    "        print(self.int_to_char)\n",
    "        # print(self.char_to_int)\n",
    "\n",
    "        self.blank_index = blank_index\n",
    "        space_index = len(labels)\n",
    "        if ' ' in labels:\n",
    "            space_index = labels.index(' ')\n",
    "        self.space_index = space_index\n",
    "\n",
    "    def greedy_decode(self, prob, digit=False):\n",
    "        # prob: [seq_len, num_labels+1], numpy array\n",
    "        indexes = np.argmax(prob, axis=1)\n",
    "        string = []\n",
    "        prev_index = -1\n",
    "        for i in range(len(indexes)):\n",
    "            if indexes[i] == self.blank_index:\n",
    "                prev_index = -1\n",
    "                continue\n",
    "            elif indexes[i] == prev_index:\n",
    "                continue\n",
    "            else:\n",
    "                if digit is False:\n",
    "                    \n",
    "                    if len(string)>1 and self.int_to_char[indexes[i]]==string[-1]:\n",
    "                        continue\n",
    "                    string.append(self.int_to_char[indexes[i]])\n",
    "                else:\n",
    "                    string.append(indexes[i])\n",
    "                prev_index = indexes[i]\n",
    "        return string\n",
    "\n",
    "    def beam_decode(self, prob, beam_size, beta=0.0, gamma=0.0, scorer=None, digit=False):\n",
    "        # prob: [seq_len, num_labels+1], numpy array\n",
    "        # beta: lm coef, gamma: insertion coef\n",
    "        seqlen = len(prob)\n",
    "        beam_idx = np.argsort(prob[0, :])[-beam_size:].tolist()\n",
    "\n",
    "        beam_prob = list(map(lambda x: math.log(prob[0, x]), beam_idx))\n",
    "        beam_idx = list(map(lambda x: [x], beam_idx))\n",
    "        for t in range(1, seqlen):\n",
    "            topk_idx = np.argsort(prob[t, :])[-beam_size:].tolist()\n",
    "            topk_prob = list(map(lambda x: prob[t, x], topk_idx))\n",
    "            aug_beam_prob, aug_beam_idx = [], []\n",
    "            for b in range(beam_size*beam_size):\n",
    "                aug_beam_prob.append(beam_prob[int(b/beam_size)])\n",
    "                aug_beam_idx.append(list(beam_idx[int(b/beam_size)]))\n",
    "            # allocate\n",
    "            for b in range(beam_size*beam_size):\n",
    "                i, j = int(b/beam_size), int(b % beam_size)\n",
    "                aug_beam_idx[b].append(topk_idx[j])\n",
    "                aug_beam_prob[b] = aug_beam_prob[b]+math.log(topk_prob[j])\n",
    "            # merge\n",
    "            merge_beam_idx, merge_beam_prob = [], []\n",
    "            for b in range(beam_size*beam_size):\n",
    "                if aug_beam_idx[b][-1] == aug_beam_idx[b][-2]:\n",
    "                    beam, beam_prob = aug_beam_idx[b][:-1], aug_beam_prob[b]\n",
    "                elif aug_beam_idx[b][-2] == self.blank_index:\n",
    "                    beam, beam_prob = aug_beam_idx[b][:-2]+[aug_beam_idx[b][-1]], aug_beam_prob[b]\n",
    "                else:\n",
    "                    beam, beam_prob = aug_beam_idx[b], aug_beam_prob[b]\n",
    "                beam_str = list(map(lambda x: self.int_to_char[x], beam))\n",
    "                if beam_str not in merge_beam_idx:\n",
    "                    merge_beam_idx.append(beam_str)\n",
    "                    merge_beam_prob.append(beam_prob)\n",
    "                else:\n",
    "                    idx = merge_beam_idx.index(beam_str)\n",
    "                    merge_beam_prob[idx] = np.logaddexp(merge_beam_prob[idx], beam_prob)\n",
    "\n",
    "            if scorer is not None:\n",
    "                merge_beam_prob_lm, ins_bonus, strings = [], [], []\n",
    "                for b in range(len(merge_beam_prob)):\n",
    "                    if merge_beam_idx[b][-1] == self.int_to_char[self.blank_index]:\n",
    "                        strings.append(merge_beam_idx[b][:-1])\n",
    "                        ins_bonus.append(len(merge_beam_idx[b][:-1]))\n",
    "                    else:\n",
    "                        strings.append(merge_beam_idx[b])\n",
    "                        ins_bonus.append(len(merge_beam_idx[b]))\n",
    "                lm_scores = scorer.get_score_fast(strings)\n",
    "                for b in range(len(merge_beam_prob)):\n",
    "                    total_score = merge_beam_prob[b]+beta*lm_scores[b]+gamma*ins_bonus[b]\n",
    "                    merge_beam_prob_lm.append(total_score)\n",
    "\n",
    "            if scorer is None:\n",
    "                ntopk_idx = np.argsort(np.array(merge_beam_prob))[-beam_size:].tolist()\n",
    "            else:\n",
    "                ntopk_idx = np.argsort(np.array(merge_beam_prob_lm))[-beam_size:].tolist()\n",
    "            beam_idx = list(map(lambda x: merge_beam_idx[x], ntopk_idx))\n",
    "            for b in range(len(beam_idx)):\n",
    "                beam_idx[b] = list(map(lambda x: self.char_to_int[x], beam_idx[b]))\n",
    "            beam_prob = list(map(lambda x: merge_beam_prob[x], ntopk_idx))\n",
    "        if self.blank_index in beam_idx[-1]:\n",
    "            pred = beam_idx[-1][:-1]\n",
    "        else:\n",
    "            pred = beam_idx[-1]\n",
    "        if digit is False:\n",
    "            pred = list(map(lambda x: self.int_to_char[x], pred))\n",
    "        return pred\n",
    "\n",
    "    # def get_trans_score(self, decoder , strings):\n",
    "\n",
    "    def beam_decode_trans(self, prob, beam_size, decoder, poses , beta=0.0, gamma=0.0, digit=False):\n",
    "        # prob: [seq_len, num_labels+1], numpy array\n",
    "        # beta: lm coef, gamma: insertion coef\n",
    "        seqlen = len(prob)\n",
    "        beam_idx = np.argsort(prob[0, :])[-beam_size:].tolist()\n",
    "\n",
    "        beam_prob = list(map(lambda x: math.log(prob[0, x]), beam_idx))\n",
    "        beam_idx = list(map(lambda x: [x], beam_idx))\n",
    "        for t in range(1, seqlen):\n",
    "            topk_idx = np.argsort(prob[t, :])[-beam_size:].tolist()\n",
    "            topk_prob = list(map(lambda x: prob[t, x], topk_idx))\n",
    "            aug_beam_prob, aug_beam_idx = [], []\n",
    "            for b in range(beam_size*beam_size):\n",
    "                aug_beam_prob.append(beam_prob[int(b/beam_size)])\n",
    "                aug_beam_idx.append(list(beam_idx[int(b/beam_size)]))\n",
    "            # allocate\n",
    "            for b in range(beam_size*beam_size):\n",
    "                i, j = int(b/beam_size), int(b % beam_size)\n",
    "                aug_beam_idx[b].append(topk_idx[j])\n",
    "                aug_beam_prob[b] = aug_beam_prob[b]+math.log(topk_prob[j])\n",
    "            # merge\n",
    "            merge_beam_idx, merge_beam_prob = [], []\n",
    "            for b in range(beam_size*beam_size):\n",
    "                if aug_beam_idx[b][-1] == aug_beam_idx[b][-2]:\n",
    "                    beam, beam_prob = aug_beam_idx[b][:-1], aug_beam_prob[b]\n",
    "                elif aug_beam_idx[b][-2] == self.blank_index:\n",
    "                    beam, beam_prob = aug_beam_idx[b][:-2]+[aug_beam_idx[b][-1]], aug_beam_prob[b]\n",
    "                else:\n",
    "                    beam, beam_prob = aug_beam_idx[b], aug_beam_prob[b]\n",
    "                beam_str = list(map(lambda x: self.int_to_char[x], beam))\n",
    "                if beam_str not in merge_beam_idx:\n",
    "                    merge_beam_idx.append(beam_str)\n",
    "                    merge_beam_prob.append(beam_prob)\n",
    "                else:\n",
    "                    idx = merge_beam_idx.index(beam_str)\n",
    "                    merge_beam_prob[idx] = np.logaddexp(merge_beam_prob[idx], beam_prob)\n",
    "\n",
    "            merge_beam_prob_lm, ins_bonus, strings = [], [], []\n",
    "            for b in range(len(merge_beam_prob)):\n",
    "                if merge_beam_idx[b][-1] == self.int_to_char[self.blank_index]:\n",
    "                    strings.append(merge_beam_idx[b][:-1])\n",
    "                    ins_bonus.append(len(merge_beam_idx[b][:-1]))\n",
    "                else:\n",
    "                    strings.append(merge_beam_idx[b])\n",
    "                    ins_bonus.append(len(merge_beam_idx[b]))\n",
    "            \n",
    "            lm_scores = decoder.return_scores(poses,strings,self.char_to_int)\n",
    "\n",
    "            for b in range(len(merge_beam_prob)):\n",
    "                total_score = merge_beam_prob[b]+beta*lm_scores[b]+gamma*ins_bonus[b]\n",
    "                merge_beam_prob_lm.append(total_score)\n",
    "\n",
    "            ntopk_idx = np.argsort(np.array(merge_beam_prob_lm))[-beam_size:].tolist()\n",
    "            \n",
    "            beam_idx = list(map(lambda x: merge_beam_idx[x], ntopk_idx))\n",
    "            for b in range(len(beam_idx)):\n",
    "                beam_idx[b] = list(map(lambda x: self.char_to_int[x], beam_idx[b]))\n",
    "            beam_prob = list(map(lambda x: merge_beam_prob[x], ntopk_idx))\n",
    "        if self.blank_index in beam_idx[-1]:\n",
    "            pred = beam_idx[-1][:-1]\n",
    "        else:\n",
    "            pred = beam_idx[-1]\n",
    "        if digit is False:\n",
    "            pred = list(map(lambda x: self.int_to_char[x], pred))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb3cfd3-0005-4397-8bee-22290444ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilities\n",
    "def remove_duplicates(x):\n",
    "    if len(x) < 2:\n",
    "        return x\n",
    "    fin = \"\"\n",
    "    for j in x:\n",
    "        if fin == \"\":\n",
    "            fin = j\n",
    "        else:\n",
    "            if j == fin[-1]:\n",
    "                continue\n",
    "            else:\n",
    "                fin = fin + j\n",
    "    return fin\n",
    "\n",
    "\n",
    "def decode_predictions(preds, encoder):\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    sign_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = []\n",
    "        for k in preds[j, :]:\n",
    "            k = k - 1\n",
    "            if k == -1:\n",
    "                temp.append(\"ยง\")\n",
    "            else:\n",
    "                p = encoder.inverse_transform([k])[0]\n",
    "                temp.append(p)\n",
    "        tp = \"\".join(temp).replace(\"ยง\", \"\")\n",
    "        sign_preds.append(remove_duplicates(tp))\n",
    "    return sign_preds\n",
    "\n",
    "def numerize(sents, vocab_map,full_transformer):\n",
    "    outs = []\n",
    "    for sent in sents:\n",
    "        if type(sent) != float :\n",
    "            if full_transformer:\n",
    "                outs.append([32]+ list(map(lambda x: vocab_map[x], sent))+ [0])\n",
    "            else:\n",
    "                outs.append(list(map(lambda x: vocab_map[x], sent)))\n",
    "\n",
    "    return outs\n",
    "\n",
    "def invert_to_chars(sents, inv_ctc_map):\n",
    "    sents = sents.detach().numpy()\n",
    "    outs = []\n",
    "    for sent in sents:\n",
    "        for x in sent:\n",
    "            if x == 0:\n",
    "                break\n",
    "            outs.append(inv_ctc_map[x]) \n",
    "    return outs\n",
    "\n",
    "def get_ctc_vocab(char_list):\n",
    "    # blank\n",
    "    ctc_char_list = \"_\" + char_list\n",
    "    ctc_map, inv_ctc_map = {}, {}\n",
    "    for i, char in enumerate(ctc_char_list):\n",
    "        ctc_map[char] = i\n",
    "        inv_ctc_map[i] = char\n",
    "    return ctc_map, inv_ctc_map, ctc_char_list\n",
    "\n",
    "def get_autoreg_vocab(char_list):\n",
    "    # blank\n",
    "    ctc_map, inv_ctc_map = {}, {}\n",
    "    for i, char in enumerate(char_list):\n",
    "        ctc_map[char] = i\n",
    "        inv_ctc_map[i] = char\n",
    "    return ctc_map, inv_ctc_map, char_list\n",
    "\n",
    "\n",
    "def convert_text_for_ctc(DATASET_CSV_PATH,vocab_map,full_transformer=False):\n",
    "    all_data = pd.read_csv(DATASET_CSV_PATH)\n",
    "    all_data = all_data[all_data['filename'].notna()]\n",
    "    all_data = all_data[all_data['label_proc'].notna()]\n",
    "    label = all_data[\"label_proc\"]\n",
    "    \n",
    "    targets_enc = numerize(label, vocab_map,full_transformer)\n",
    "\n",
    "    # targets = [[c for c in x] for x in label]\n",
    "    # targets_flat = [c for clist in targets for c in clist]\n",
    "    # lbl_enc = preprocessing.LabelEncoder()\n",
    "    # lbl_enc.fit(targets_flat)\n",
    "    # targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    # targets_enc = np.array(targets_enc)\n",
    "    # targets_enc = targets_enc + 1\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"names\"] = all_data[\"filename\"]\n",
    "    df[\"enc\"] = targets_enc\n",
    "\n",
    "    # print(\"number of classes after conversion for CTC\", lbl_enc.classes_)\n",
    "    \n",
    "    return  df\n",
    "\n",
    "    # return  df , lbl_enc\n",
    "\n",
    "\n",
    "subs = np.zeros((26,26))\n",
    "\n",
    "def iterative_levenshtein(s, t, costs=(1, 1, 1)):\n",
    "    \"\"\" \n",
    "    Computes Levenshtein distance between the strings s and t.\n",
    "    For all i and j, dist[i,j] will contain the Levenshtein \n",
    "    distance between the first i characters of s and the \n",
    "    first j characters of t\n",
    "    s: source, t: target\n",
    "    costs: a tuple or a list with three integers (d, i, s)\n",
    "           where d defines the costs for a deletion\n",
    "                 i defines the costs for an insertion and\n",
    "                 s defines the costs for a substitution\n",
    "    return: \n",
    "    H, S, D, I: correct chars, number of substitutions, number of deletions, number of insertions\n",
    "    \"\"\"\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    deletes, inserts, substitutes = costs\n",
    "    \n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "    H, D, S, I = 0, 0, 0, 0\n",
    "    for row in range(1, rows):\n",
    "        dist[row][0] = row * deletes\n",
    "    for col in range(1, cols):\n",
    "        dist[0][col] = col * inserts\n",
    "        \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = substitutes\n",
    "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
    "                                 dist[row][col-1] + inserts,\n",
    "                                 dist[row-1][col-1] + cost)\n",
    "    row, col = rows-1, cols-1\n",
    "    while row != 0 or col != 0:\n",
    "        if row == 0:\n",
    "            I += col\n",
    "            col = 0\n",
    "        elif col == 0:\n",
    "            D += row\n",
    "            row = 0\n",
    "        elif dist[row][col] == dist[row-1][col] + deletes:\n",
    "            D += 1\n",
    "            row = row-1\n",
    "        elif dist[row][col] == dist[row][col-1] + inserts:\n",
    "            I += 1\n",
    "            col = col-1\n",
    "        elif dist[row][col] == dist[row-1][col-1] + substitutes:\n",
    "            S += 1\n",
    "            row, col = row-1, col-1\n",
    "\n",
    "            print(s,t, s[row],t[col])\n",
    "            if s[row] not in [' ','.'] and t[col] not in [' ','.'] :\n",
    "                subs[ord(s[row])-97][ord(t[col])-97] += 1\n",
    "\n",
    "        else:\n",
    "            H += 1\n",
    "            row, col = row-1, col-1\n",
    "    D, I = I, D\n",
    "    # print()\n",
    "    return H, D, S, I\n",
    "\n",
    "def compute_acc(preds, labels, costs=(7, 7, 10)):\n",
    "    # cost according to HTK: http://www.ee.columbia.edu/~dpwe/LabROSA/doc/HTKBook21/node142.html\n",
    "\n",
    "    if not len(preds) == len(labels):\n",
    "        raise ValueError('# predictions not equal to # labels')\n",
    "    Ns, Ds, Ss, Is = 0, 0, 0, 0\n",
    "    for i, _ in enumerate(preds):\n",
    "        H, D, S, I = iterative_levenshtein(preds[i], labels[i], costs)\n",
    "        # print(H, D, S, I)\n",
    "        Ns += len(labels[i])\n",
    "        Ds += D\n",
    "        Ss += S\n",
    "        Is += I\n",
    "    try:\n",
    "        acc = 100*(Ns-Ds-Ss-Is)/Ns\n",
    "    except ZeroDivisionError as err:\n",
    "        raise ZeroDivisionError('Empty labels')\n",
    "    \n",
    "    print(Ds, Ss, Is, Ns)\n",
    "    print(subs)\n",
    "    return acc\n",
    "\n",
    "# compute_acc([\"akbyr\"],[\"aaaabkbar\"])\n",
    "\n",
    "def ctcloss_reference(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='none', logits_lm =None ):\n",
    "    input_lengths = torch.as_tensor(input_lengths, dtype=torch.long)\n",
    "    target_lengths = torch.as_tensor(target_lengths, dtype=torch.long)\n",
    "    dt = log_probs.dtype\n",
    "    log_probs = log_probs.double()  # we need the accuracy as we are not in logspace\n",
    "    targets = targets.long()\n",
    "    cum_target_lengths = target_lengths.cumsum(0)\n",
    "    losses = []\n",
    "    for i in range(log_probs.size(1)):\n",
    "        input_length = input_lengths[i].item()\n",
    "        target_length = target_lengths[i].item()\n",
    "        cum_target_length = cum_target_lengths[i].item()\n",
    "        # ==========================================================================================================\n",
    "        targets_prime = targets.new_full((2 * target_length + 1,), blank)\n",
    "        if targets.dim() == 2:\n",
    "            targets_prime[1::2] = targets[i, :target_length]\n",
    "        else:\n",
    "            targets_prime[1::2] = targets[cum_target_length - target_length:cum_target_length]\n",
    "        # ==========================================================================================================\n",
    "        probs = log_probs[:input_length, i].exp()\n",
    "        # ==========================================================================================================\n",
    "        alpha = log_probs.new_zeros((target_length * 2 + 1,))\n",
    "        logits_lm = F.softmax(logits_lm, dim=-1)\n",
    "        \n",
    "        lm_alpha = log_probs.new_zeros((target_length * 2 + 1,),dtype=logits_lm.dtype).float()\n",
    "        lm_alpha[1::2] = torch.diagonal(logits_lm[:,targets[0]], 0) \n",
    "\n",
    "        alpha[0] = probs[0, blank]\n",
    "        alpha[1] = probs[0, targets_prime[1]]\n",
    "        mask_third = (targets_prime[:-2] != targets_prime[2:])\n",
    "        \n",
    "        for t in range(1, input_length):\n",
    "            alpha_next = alpha.clone()\n",
    "            alpha_next[1:] += (alpha[:-1] + lm_alpha[1:])\n",
    "            alpha_next[2:] += torch.where(mask_third, alpha[:-2], alpha.new_zeros(1))\n",
    "            alpha = probs[t, targets_prime] * alpha_next\n",
    "            # if logits_lm != None:\n",
    "        # ==========================================================================================================\n",
    "        losses.append(-alpha[-2:].sum().log()[None])\n",
    "    output = torch.cat(losses, 0)\n",
    "    if reduction == 'mean':\n",
    "        return (output / target_lengths.to(dtype=output.dtype, device=output.device)).mean()\n",
    "    elif reduction == 'sum':\n",
    "        return output.sum()\n",
    "    output = output.to(dt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5fb469-28ad-4f2b-9e53-1ca8a8dabf8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab_map, inv_vocab_map, char_list \u001b[38;5;241m=\u001b[39m get_autoreg_vocab(\u001b[43mchars\u001b[49m)\n\u001b[1;32m      2\u001b[0m decoder_dec \u001b[38;5;241m=\u001b[39m Decoder(char_list, blank_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(vocab_map)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chars' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_map, inv_vocab_map, char_list = get_autoreg_vocab(chars)\n",
    "decoder_dec = Decoder(char_list, blank_index=0)\n",
    "\n",
    "print(vocab_map)\n",
    "print(inv_vocab_map)\n",
    "print(char_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e16231-8e6c-4e5f-8b55-261882d58b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_enc_df = convert_text_for_ctc(labels_csv,vocab_map,True)\n",
    "\n",
    "transform = transforms.Compose([GaussianNoise()])\n",
    "\n",
    "dataset_train = HandPoseDataset(data_dir, labels_csv ,hand_detected_label, target_enc_df , \"train\", transform=transform)\n",
    "traindataloader = DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "dataset_test = HandPoseDataset(data_dir, labels_csv , hand_detected_label, target_enc_df , \"test\" , augmentations =False )\n",
    "testdataloader = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "model = TransformerModel(output_dim=len(char_list), d_input = 42 ,d_model=256, nhead=8, num_layers=3, dropout=0.1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9199e0-aeed-4919-9f00-890b94fa9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_encoder = nn.CTCLoss(blank=0,zero_infinity=True, reduction='none')\n",
    "loss_decoder = nn.CrossEntropyLoss()\n",
    "# loss_decoder = nn.NLLLoss()\n",
    "loss_cls = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a09bdc-f1e7-43ae-8de1-c0a254e6ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer, step_size=optim_step_size, gamma=optim_gamma)\n",
    "\n",
    "vocab_map_enc, inv_vocab_map_enc, char_list_enc = get_ctc_vocab(chars[1:])\n",
    "decoder_enc = Decoder(char_list_enc, blank_index=0)\n",
    "print(vocab_map_enc)\n",
    "print(inv_vocab_map_enc)\n",
    "print(char_list_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8579f-fd4e-4a7c-946a-e42d1dd56f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc695ca0-aa81-4f13-82f8-632c38385cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_loss_cls = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_loss_cls = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (poses, labels) in enumerate(traindataloader):\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        cls_token , logits_lm, encoder_out = model(poses.to(device), labels[:, :-1].to(device))\n",
    "\n",
    "        log_probs_enc = F.log_softmax(encoder_out, dim=-1).permute(1,0,2)\n",
    "        \n",
    "        input_lengths = torch.full((encoder_out.size(0),), log_probs_enc.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full((encoder_out.size(0),), labels.size(1)-2, dtype=torch.long )\n",
    "        \n",
    "        loss_enc = loss_encoder(log_probs_enc, labels[:,1:-1], input_lengths=input_lengths, target_lengths=target_lengths)\n",
    "        \n",
    "        # print(log_probs_enc.device,input_lengths.device)\n",
    "\n",
    "        # loss_enc = ctcloss_reference(log_probs_enc, labels[:,1:-1].cuda(), input_lengths, target_lengths, logits_lm= logits_lm[0,:-1]).float()\n",
    "        \n",
    "        # print(loss_enc,expected)\n",
    "\n",
    "        loss_dec = loss_decoder(logits_lm[0].cpu(), labels[:,1:].view(-1))\n",
    "\n",
    "\n",
    "        # if i%30 == 0:\n",
    "        #     current_preds_enc = decoder_enc.greedy_decode(log_probs_enc[:,0,:].detach().cpu().numpy())\n",
    "        #     current_preds_enc = ''.join(current_preds_enc)\n",
    "        #     print(current_preds_enc, ''.join(invert_to_chars(labels[:,1:-1],inv_vocab_map)))\n",
    "        \n",
    "        gt_label_size = torch.tensor([[math.sin(((labels.size(1)-2)/30-0.5)*2*torch.pi),math.cos(((labels.size(1)-2)/30-0.5)*2*torch.pi)]],device = device )\n",
    "        loss_token = (cls_token, gt_label_size) \n",
    "\n",
    "        loss = loss_dec+ 5*loss_enc + loss_token\n",
    "\n",
    "        if i%400 == 0:    \n",
    "            print('Epoch {}/{} - loss: {:.4f} - loss_cls: {:.4f}' .format(epoch+1, num_epochs, total_loss/(i+1), total_loss_cls/(i+1)))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_loss_cls += loss_token.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gt_labels = []\n",
    "\n",
    "    if epoch < 9 :\n",
    "        continue\n",
    "    \n",
    "\n",
    "    for i, (poses, labels) in enumerate(testdataloader):\n",
    "        poses = poses.to(device)\n",
    "        cls_token , logits = model(poses)\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        pred_size = (torch.atan2(torch.tensor([cls_token[0,0].detach().cpu()]),torch.tensor([cls_token[0,1].detach().cpu()]))/(2 * torch.pi) +0.5) * 30\n",
    "        pred_size = torch.round(pred_size)\n",
    "\n",
    "        current_preds = decoder_dec.beam_decode_trans(log_probs[0].detach().cpu(), beam_size, model, poses , beta=lm_beta, gamma=ins_gamma)\n",
    "        current_preds = ''.join(current_preds)\n",
    "\n",
    "        preds.append(current_preds)\n",
    "\n",
    "        print(current_preds,''.join(invert_to_chars(labels[:,1:-1],inv_vocab_map)), \"   \", pred_size) \n",
    "        gt_labels.append(''.join(invert_to_chars(labels[:,1:-1],inv_vocab_map)))\n",
    "\n",
    "    lev_acc = compute_acc(preds, gt_labels)\n",
    "    if best_acc < lev_acc:\n",
    "        best_acc = lev_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "\n",
    "    print('Epoch {}/{} - Letter Acc: {:.4f} - Best Acc {:.4f}'.format(epoch+1, num_epochs, lev_acc, best_acc))    model.train()\n",
    "\n",
    "    for i, (poses, labels) in enumerate(traindataloader):\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        cls_token , logits_lm, encoder_out = model(poses.to(device), labels[:, :-1].to(device))\n",
    "\n",
    "        log_probs_enc = F.log_softmax(encoder_out, dim=-1).permute(1,0,2)\n",
    "        \n",
    "        input_lengths = torch.full((encoder_out.size(0),), log_probs_enc.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full((encoder_out.size(0),), labels.size(1)-2, dtype=torch.long )\n",
    "        \n",
    "        loss_enc = loss_encoder(log_probs_enc, labels[:,1:-1], input_lengths=input_lengths, target_lengths=target_lengths)\n",
    "        \n",
    "        # print(log_probs_enc.device,input_lengths.device)\n",
    "\n",
    "        # loss_enc = ctcloss_reference(log_probs_enc, labels[:,1:-1].cuda(), input_lengths, target_lengths, logits_lm= logits_lm[0,:-1]).float()\n",
    "        \n",
    "        # print(loss_enc,expected)\n",
    "\n",
    "        loss_dec = loss_decoder(logits_lm[0].cpu(), labels[:,1:].view(-1))\n",
    "\n",
    "\n",
    "        # if i%30 == 0:\n",
    "        #     current_preds_enc = decoder_enc.greedy_decode(log_probs_enc[:,0,:].detach().cpu().numpy())\n",
    "        #     current_preds_enc = ''.join(current_preds_enc)\n",
    "        #     print(current_preds_enc, ''.join(invert_to_chars(labels[:,1:-1],inv_vocab_map)))\n",
    "        \n",
    "        gt_label_size = torch.tensor([[math.sin(((labels.size(1)-2)/30-0.5)*2*torch.pi),math.cos(((labels.size(1)-2)/30-0.5)*2*torch.pi)]],device = device )\n",
    "        loss_token = (cls_token, gt_label_size) \n",
    "\n",
    "        loss = loss_dec+ 5*loss_enc + loss_token\n",
    "\n",
    "        if i%400 == 0:    \n",
    "            print('Epoch {}/{} - loss: {:.4f} - loss_cls: {:.4f}' .format(epoch+1, num_epochs, total_loss/(i+1), total_loss_cls/(i+1)))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_loss_cls += loss_token.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gt_labels = []\n",
    "\n",
    "    if epoch < 9 :\n",
    "        continue\n",
    "    \n",
    "\n",
    "    for i, (poses, labels) in enumerate(testdataloader):\n",
    "        poses = poses.to(device)\n",
    "        cls_token , logits = model(poses)\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        pred_size = (torch.atan2(torch.tensor([cls_token[0,0].detach().cpu()]),torch.tensor([cls_token[0,1].detach().cpu()]))/(2 * torch.pi) +0.5) * 30\n",
    "        pred_size = torch.round(pred_size)\n",
    "\n",
    "        current_preds = decoder_dec.beam_decode_trans(log_probs[0].detach().cpu(), beam_size, model, poses , beta=lm_beta, gamma=ins_gamma)\n",
    "        current_preds = ''.join(current_preds)\n",
    "\n",
    "        preds.append(current_preds)\n",
    "\n",
    "        print(current_preds,''.join(invert_to_chars(labels[:,1:-1],inv_vocab_map)), \"   \", pred_size) \n",
    "        gt_labels.append(''.join(invert_to_chars(labels[:,1:-1],inv_vocab_map)))\n",
    "\n",
    "    lev_acc = compute_acc(preds, gt_labels)\n",
    "    if best_acc < lev_acc:\n",
    "        best_acc = lev_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "\n",
    "    print('Epoch {}/{} - Letter Acc: {:.4f} - Best Acc {:.4f}'.format(epoch+1, num_epochs, lev_acc, best_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
